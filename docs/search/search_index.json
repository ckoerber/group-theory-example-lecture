{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lecture based on reveal.js This repository contains slides presented in the weekly \"Group Theory in Nuclear and Particle Physics\" lecture (the 10th lecture) at Ruhr-University Bochum during the winter semester of 21/22. The lecture is a master's level course for physicists and mathematicians. In this documentation, you can find details and brief instructions about the used packages, the concept behind the slides and customizations made to match the weekly lecture schedule Modifications These slides are based on reveal.js . The repository is already configured to load dependencies and style files automatically. This configuration is implemented by the index.html file, which loads the content of the slides via the <section data-markdown=\"slides/lecture-10/*.html\"> tags. The source code for individual slides can be found in the slides/lecture-10/*.html files. If you want to update the slides, adjust the respective slide HTML. If you want to add or remove slides, adjust or add corresponding section tags in the index.html file. It is also possible to load markdown files instead of HTML files. Both the markdown and HTML versions can render latex (wrapped with a single $ for inline latex and a double $$ for equation environments). In the language of reveal.js, individual slides are called <sections> (html tags) and animated objects within slides are called class=\"fragment\" (html attributes). Contents like headings, paragraphs, lists, and images follow the standard HTML or markdown syntax. Run it The slides can be viewed on a local machine without installing dependencies; they are already included in the static folder (no additional npm install is needed). I.e., the presentation can be directly viewed in any browser ( index.html ) after cloning the repository. Because of the current file layout (slides are separated into several files and included by index.html ), to properly access files, you have to launch a local HTTP server with python3 -m http.server # or npx http-server and visiting http://{ip-address}:8000/ (where the IP-address is usually 127.0.0.1 , localhost , or 0.0.0.0 ; the above command tells you where to look). This server dependence can be circumvented by removing the data-markdown=\"slides/lecture-10/0-about.html\" attributes in index.html and, instead, placing the file's content inside the <section> tags. Components Note It was the design choice to place external dependencies rather on the creators' side so that the users (clients) only need to load a minimal set of dependencies\u2014improving stability. The scripts and ideas explained in the following sections were used on macOS and Ubuntu systems. Some of the helper scripts used Python3.8. They are not thoroughly tested and should be viewed as an example for accomplishing the goal. I am happy to receive feedback and PRs which improve these steps. Creating audio files and the voice-over Recording and embedding videos Animating vector graphics Custom CSS and visual improvements Embedding slides into other frameworks (like Moodle)","title":"Home"},{"location":"#lecture-based-on-revealjs","text":"This repository contains slides presented in the weekly \"Group Theory in Nuclear and Particle Physics\" lecture (the 10th lecture) at Ruhr-University Bochum during the winter semester of 21/22. The lecture is a master's level course for physicists and mathematicians. In this documentation, you can find details and brief instructions about the used packages, the concept behind the slides and customizations made to match the weekly lecture schedule","title":"Lecture based on reveal.js"},{"location":"#modifications","text":"These slides are based on reveal.js . The repository is already configured to load dependencies and style files automatically. This configuration is implemented by the index.html file, which loads the content of the slides via the <section data-markdown=\"slides/lecture-10/*.html\"> tags. The source code for individual slides can be found in the slides/lecture-10/*.html files. If you want to update the slides, adjust the respective slide HTML. If you want to add or remove slides, adjust or add corresponding section tags in the index.html file. It is also possible to load markdown files instead of HTML files. Both the markdown and HTML versions can render latex (wrapped with a single $ for inline latex and a double $$ for equation environments). In the language of reveal.js, individual slides are called <sections> (html tags) and animated objects within slides are called class=\"fragment\" (html attributes). Contents like headings, paragraphs, lists, and images follow the standard HTML or markdown syntax.","title":"Modifications"},{"location":"#run-it","text":"The slides can be viewed on a local machine without installing dependencies; they are already included in the static folder (no additional npm install is needed). I.e., the presentation can be directly viewed in any browser ( index.html ) after cloning the repository. Because of the current file layout (slides are separated into several files and included by index.html ), to properly access files, you have to launch a local HTTP server with python3 -m http.server # or npx http-server and visiting http://{ip-address}:8000/ (where the IP-address is usually 127.0.0.1 , localhost , or 0.0.0.0 ; the above command tells you where to look). This server dependence can be circumvented by removing the data-markdown=\"slides/lecture-10/0-about.html\" attributes in index.html and, instead, placing the file's content inside the <section> tags.","title":"Run it"},{"location":"#components","text":"Note It was the design choice to place external dependencies rather on the creators' side so that the users (clients) only need to load a minimal set of dependencies\u2014improving stability. The scripts and ideas explained in the following sections were used on macOS and Ubuntu systems. Some of the helper scripts used Python3.8. They are not thoroughly tested and should be viewed as an example for accomplishing the goal. I am happy to receive feedback and PRs which improve these steps. Creating audio files and the voice-over Recording and embedding videos Animating vector graphics Custom CSS and visual improvements Embedding slides into other frameworks (like Moodle)","title":"Components"},{"location":"about/","text":"About Because of the COVID-19 Pandemic, it was not feasible to have in-person (university) lectures during the entire year of 2021. Remote lectures were Zoom meetings. On first thought, the two most common options for presenting theoretical knowledge are white-board like lectures where the majority of the meeting uses handwritten visuals, and the presentation of slides. Depending on the content, both have down- and upsides. So, why shouldn't one use both? Having the demands that: The lectures should be reasonably standalone. I.e., accessibly both in \"live\" events and \"asynchronously\" on learning platforms (like Moodle). The asynchronous part should be interactive and searchable. The lectures should contain dynamic elements (videos, intractable graphs, quizzes, ...). The lectures should be under version control and editable by the Teaching Assistants (i.e., platform-independent and uses open-source stacks; with a strong emphasis here) While LaTeX/Beamer fulfills most of the points (though I find it relatively hard to obtain \"visual beauty\"), it does not allow dynamic components; so one would have to develop and deploy further resources. While Keynote and PowerPoint fulfill the first points, they are unfortunately not open-source, and it is not useful to place related files under version control (plus they have a relatively large memory footprint). This eventually brought me to reveal.js \u2014 combining all of the above aspects; I would describe it as the dynamic web-equivalent of beamer. Arguably, creating such material is a lot of work in the first place. However, I believe that if properly developed over multiple iterations, such material ensures a better education for future students and may even save preparation time in the long run.","title":"About"},{"location":"about/#about","text":"Because of the COVID-19 Pandemic, it was not feasible to have in-person (university) lectures during the entire year of 2021. Remote lectures were Zoom meetings. On first thought, the two most common options for presenting theoretical knowledge are white-board like lectures where the majority of the meeting uses handwritten visuals, and the presentation of slides. Depending on the content, both have down- and upsides. So, why shouldn't one use both? Having the demands that: The lectures should be reasonably standalone. I.e., accessibly both in \"live\" events and \"asynchronously\" on learning platforms (like Moodle). The asynchronous part should be interactive and searchable. The lectures should contain dynamic elements (videos, intractable graphs, quizzes, ...). The lectures should be under version control and editable by the Teaching Assistants (i.e., platform-independent and uses open-source stacks; with a strong emphasis here) While LaTeX/Beamer fulfills most of the points (though I find it relatively hard to obtain \"visual beauty\"), it does not allow dynamic components; so one would have to develop and deploy further resources. While Keynote and PowerPoint fulfill the first points, they are unfortunately not open-source, and it is not useful to place related files under version control (plus they have a relatively large memory footprint). This eventually brought me to reveal.js \u2014 combining all of the above aspects; I would describe it as the dynamic web-equivalent of beamer. Arguably, creating such material is a lot of work in the first place. However, I believe that if properly developed over multiple iterations, such material ensures a better education for future students and may even save preparation time in the long run.","title":"About"},{"location":"components/audio/","text":"Adding an audio voice-over This section explains how to add an audio voice-over to the slides (or how to disable it). Used tools Audacity scripts/label.py (Already included in the presentation by default) reveal.js-plugins/audio-slideshow and static/js/timer.js Concept When recording the lecture, either using Audacity or Zoom, I create a single mp3 file for the entire recording. The custom JavaScript static/js/timer.js logs whenever I advance a slide (right and down). This log can be imported into Audacity to identify reveal.js slides/fragments with sections of the recording (i.e., labeling the sound track). The individually labeled track can be exported to multiple mp3 files, which are automatically detected by the presentation. This modularity allows replacing or modifying individual parts at a later point. (In principle, reveal.js-plugins/audio-slideshow adds a custom recorder to the browser with similar functionality.) Steps Creating and linking the audio files to the presentation works in two steps. First, you have to create a presentation without the audio. Second, you have to record the audio and slide transitions (\"Live\"). Third, you have to combine the information regarding slide transitions with the recorded audio file (\"Post-Processing\"). Live Start a recording of the presentation. Open the slides and press the s key to reset the timer. Finish the presentation. Press the d key to download the log ( it essential you download the log before closing the browser/tab or pressing the s key again ). Post-processing Parse the downloaded CSV file into the Audacity readable format by running python scripts/label.py [--coord \"{timestamp}->{slide}\"] log.csv This script creates the file log.csv.auda.txt , which can be imported into Audacity. The optional --coord flag allows associating a given timestamp with a given slide/fragment position. I.e., --coord \"00:01:30->2.0\" specifies that the 1min 30s mark should correspond to slide transition 2.0 . The slide positions and fragments can be read of the browser URL, but note that they are offset by one. Once the *.auda.txt label file was created, you can manually adjust entries; i.e., remove unwanted transitions and adjust individual timings by editing the txt file (for example, in case of some transitions, in the beginning, should not be present). Import the audio file File > Import > Audio and label file File > Import > Labels into Audacity. Once you are happy with the label positions, export the individual files with File > Export > Export Multiple... . It is important to select: \"Split files based on Labels\" \"Name files using Labels/Track Name\" and select the folder media/audio/main (can be changed) The created audio files now correspond to the proper slide positions and are automatically detected by audio-slideshow . Configuration The configuration of the auto-slideshow is handled in static/js/main-w-audio.js . See also reveal.js-plugins/audio-slideshow for more options.","title":"Audio"},{"location":"components/audio/#adding-an-audio-voice-over","text":"This section explains how to add an audio voice-over to the slides (or how to disable it).","title":"Adding an audio voice-over"},{"location":"components/audio/#used-tools","text":"Audacity scripts/label.py (Already included in the presentation by default) reveal.js-plugins/audio-slideshow and static/js/timer.js","title":"Used tools"},{"location":"components/audio/#concept","text":"When recording the lecture, either using Audacity or Zoom, I create a single mp3 file for the entire recording. The custom JavaScript static/js/timer.js logs whenever I advance a slide (right and down). This log can be imported into Audacity to identify reveal.js slides/fragments with sections of the recording (i.e., labeling the sound track). The individually labeled track can be exported to multiple mp3 files, which are automatically detected by the presentation. This modularity allows replacing or modifying individual parts at a later point. (In principle, reveal.js-plugins/audio-slideshow adds a custom recorder to the browser with similar functionality.)","title":"Concept"},{"location":"components/audio/#steps","text":"Creating and linking the audio files to the presentation works in two steps. First, you have to create a presentation without the audio. Second, you have to record the audio and slide transitions (\"Live\"). Third, you have to combine the information regarding slide transitions with the recorded audio file (\"Post-Processing\").","title":"Steps"},{"location":"components/audio/#live","text":"Start a recording of the presentation. Open the slides and press the s key to reset the timer. Finish the presentation. Press the d key to download the log ( it essential you download the log before closing the browser/tab or pressing the s key again ).","title":"Live"},{"location":"components/audio/#post-processing","text":"Parse the downloaded CSV file into the Audacity readable format by running python scripts/label.py [--coord \"{timestamp}->{slide}\"] log.csv This script creates the file log.csv.auda.txt , which can be imported into Audacity. The optional --coord flag allows associating a given timestamp with a given slide/fragment position. I.e., --coord \"00:01:30->2.0\" specifies that the 1min 30s mark should correspond to slide transition 2.0 . The slide positions and fragments can be read of the browser URL, but note that they are offset by one. Once the *.auda.txt label file was created, you can manually adjust entries; i.e., remove unwanted transitions and adjust individual timings by editing the txt file (for example, in case of some transitions, in the beginning, should not be present). Import the audio file File > Import > Audio and label file File > Import > Labels into Audacity. Once you are happy with the label positions, export the individual files with File > Export > Export Multiple... . It is important to select: \"Split files based on Labels\" \"Name files using Labels/Track Name\" and select the folder media/audio/main (can be changed) The created audio files now correspond to the proper slide positions and are automatically detected by audio-slideshow .","title":"Post-processing"},{"location":"components/audio/#configuration","text":"The configuration of the auto-slideshow is handled in static/js/main-w-audio.js . See also reveal.js-plugins/audio-slideshow for more options.","title":"Configuration"},{"location":"components/custom-css/","text":"Currently under construction","title":"Currently under construction"},{"location":"components/custom-css/#currently-under-construction","text":"","title":"Currently under construction"},{"location":"components/embedding/","text":"Moodle integration Reveal.js integration can be directly embedded into Moodle via the file feature. Used tools Moodle (Optional) the npm convenience script npm run zip:prod uses some bash tools to streamline the creation of a ZIP file needed for the upload. Steps Zip the entire reveal.js presentation (including media, static, and HTML files). Create a file tool in the Moodle course and upload the zip. Unzip the uploaded zip folder in the Moodle tool and specify the index.html to be the main file. The link to this Moodle file resource will now display the presentation. Thank you to the RUB eLearning support team for the helpful instructions! [Optional] Inline embedding This resource can also be embedded as an <iframe> to be viewed inline. I.e., create a Label resource and add the following HTML <iframe width=\"850\" height=\"600\" src=\"https://moodle.{link-to-created-resource}/index.html#/\"> Fallback text here for unsupporting browsers, of which there are scant few. </iframe> Scripts You can also run the npm run zip:prod command to compile static files and create a zip containing the entire page for the upload. Depending on the modifications you have introduced, you may have to update the logic. Note I am currently trending towards formulating this as a traditional Makefile .","title":"Moodle"},{"location":"components/embedding/#moodle-integration","text":"Reveal.js integration can be directly embedded into Moodle via the file feature.","title":"Moodle integration"},{"location":"components/embedding/#used-tools","text":"Moodle (Optional) the npm convenience script npm run zip:prod uses some bash tools to streamline the creation of a ZIP file needed for the upload.","title":"Used tools"},{"location":"components/embedding/#steps","text":"Zip the entire reveal.js presentation (including media, static, and HTML files). Create a file tool in the Moodle course and upload the zip. Unzip the uploaded zip folder in the Moodle tool and specify the index.html to be the main file. The link to this Moodle file resource will now display the presentation. Thank you to the RUB eLearning support team for the helpful instructions!","title":"Steps"},{"location":"components/embedding/#optional-inline-embedding","text":"This resource can also be embedded as an <iframe> to be viewed inline. I.e., create a Label resource and add the following HTML <iframe width=\"850\" height=\"600\" src=\"https://moodle.{link-to-created-resource}/index.html#/\"> Fallback text here for unsupporting browsers, of which there are scant few. </iframe>","title":"[Optional] Inline embedding"},{"location":"components/embedding/#scripts","text":"You can also run the npm run zip:prod command to compile static files and create a zip containing the entire page for the upload. Depending on the modifications you have introduced, you may have to update the logic. Note I am currently trending towards formulating this as a traditional Makefile .","title":"Scripts"},{"location":"components/figures/","text":"Animating vector graphics This section explains a possible way how you can build up vector graphics like on Slide 4/2 in the example lecture . It assumes you already have created or have access to a vector graphic (i.e., a PDF or SVG). Used Tools Inkscape (but any other tool to form SVG layers should work as well) scripts/layer2svg.py (i.e., Python3) Concept To me, the most stable solution for animating figures is to include figure elements as individual fragments. This solution does not require additional client-side dependencies and allows a stable PDF export. Steps The essential steps to animate the graphics are: Import the vector graphic to Inkscape. Move animation steps to individual layers. Export the layered figure to SVG . Run scripts/layer2svg.py to create individual SVG files for each layer. Include and adjust the corresponding HTML inside your slides. Move animation steps to individual layers in Inkscape Once a file was imported to Inkscape ( File > Open ), the steps are: Open the Layers tab via Layer > Layers . Create a new layer for the next animation step, Layer > Add layer . Select all objects you do not want on the first animation step and move them to the new layer ( Layer> Move Selection to Layer Above ). Depending on the input, it might not directly be possible to select some objects alone; you may have to ungroup them first Object > Ungroup (or you may want to group them for easier handling Object > Group ). Repeat this procedure until you have separated all objects onto their respective animation layer Export to SVG by File > Save as and select the SVG format Create individual SVG files for each layer For convenience, I have provided a script (inspired by github.com/james-bird/layer-to-svg/ ) which exports the newly created layers into their own SVG . I.e., running python3 scripts/layer2svg.py [-n 0] media/imgs/figure.svg will create the separated SVG files in media/imgs/figure/layer1.svg media/imgs/figure/layer2.svg ... and also prints the corresponding reveal.js HTML <div class=\"r-stack\"> <img class=\"fragment\" data-fragment-index=\"0\" src=\"media/imgs/layer1.svg\"> <img class=\"fragment\" data-fragment-index=\"1\" src=\"media/imgs/layer1.svg\"> ... </div> which can be directly included in the slides. The -n option allows specifying the starting data-fragment-index . The data-fragment-index can be omitted but may make it easier to coordinate text and graphic appearance orders in the slides.","title":"Figures"},{"location":"components/figures/#animating-vector-graphics","text":"This section explains a possible way how you can build up vector graphics like on Slide 4/2 in the example lecture . It assumes you already have created or have access to a vector graphic (i.e., a PDF or SVG).","title":"Animating vector graphics"},{"location":"components/figures/#used-tools","text":"Inkscape (but any other tool to form SVG layers should work as well) scripts/layer2svg.py (i.e., Python3)","title":"Used Tools"},{"location":"components/figures/#concept","text":"To me, the most stable solution for animating figures is to include figure elements as individual fragments. This solution does not require additional client-side dependencies and allows a stable PDF export.","title":"Concept"},{"location":"components/figures/#steps","text":"The essential steps to animate the graphics are: Import the vector graphic to Inkscape. Move animation steps to individual layers. Export the layered figure to SVG . Run scripts/layer2svg.py to create individual SVG files for each layer. Include and adjust the corresponding HTML inside your slides.","title":"Steps"},{"location":"components/figures/#move-animation-steps-to-individual-layers-in-inkscape","text":"Once a file was imported to Inkscape ( File > Open ), the steps are: Open the Layers tab via Layer > Layers . Create a new layer for the next animation step, Layer > Add layer . Select all objects you do not want on the first animation step and move them to the new layer ( Layer> Move Selection to Layer Above ). Depending on the input, it might not directly be possible to select some objects alone; you may have to ungroup them first Object > Ungroup (or you may want to group them for easier handling Object > Group ). Repeat this procedure until you have separated all objects onto their respective animation layer Export to SVG by File > Save as and select the SVG format","title":"Move animation steps to individual layers in Inkscape"},{"location":"components/figures/#create-individual-svg-files-for-each-layer","text":"For convenience, I have provided a script (inspired by github.com/james-bird/layer-to-svg/ ) which exports the newly created layers into their own SVG . I.e., running python3 scripts/layer2svg.py [-n 0] media/imgs/figure.svg will create the separated SVG files in media/imgs/figure/layer1.svg media/imgs/figure/layer2.svg ... and also prints the corresponding reveal.js HTML <div class=\"r-stack\"> <img class=\"fragment\" data-fragment-index=\"0\" src=\"media/imgs/layer1.svg\"> <img class=\"fragment\" data-fragment-index=\"1\" src=\"media/imgs/layer1.svg\"> ... </div> which can be directly included in the slides. The -n option allows specifying the starting data-fragment-index . The data-fragment-index can be omitted but may make it easier to coordinate text and graphic appearance orders in the slides.","title":"Create individual SVG files for each layer"},{"location":"components/video/","text":"Adding video recordings to slides This section explains how the \"derivation videos\" were created and added to the slides. Used tools Writing hardware. I.e., a graphics tablet or similar (I used an IPad + Apple Pen with Notability ) A screen recorder (i.e., QuickTime on macOS, screencast on Ubuntu or Zoom) A way to connect the writer with the recorder (I used UxPlay to duplicate the IPad screen on Ubuntu) (Optional) ffmpeg or other tools to edit the video. (Optional) scripts/vmanip.py (Python) to generate ffmpeg commands for cropping the created video. Steps Setup the hardware and recoding software and record the video :) Cropping the video The script scripts/vmanip.py allows generating ffmpeg commands which can be used to optimize and crop the video. To continue, you first need to install ffmpeg and pip install -r requirements.txt . The command streamlit run scripts/vmanip.py will create a GUI which can be accessed in your browser (see the printed URL). When following the URL, you can specify a movie, drag-and-drop the boundaries to crop the file, specify a factor to speed up the recording, and eventually copy the corresponding ffmpeg command. For example, for a crop to the box width:height:x:y and a speedup of x1.05 ffmpeg -i media/videos/lecture-10/su2-example.mp4 -filter_complex \"[0]setpts=0.95*PTS[b];[b] crop = 3385:2109:37:28[c];[0]atempo=1.05[a]\" -map \"[c]\" -map \"[a]\" media/videos/lecture-10/su2-example.cropped.mp4 This command needs to be run outside the GUI to create the modified file.","title":"Video"},{"location":"components/video/#adding-video-recordings-to-slides","text":"This section explains how the \"derivation videos\" were created and added to the slides.","title":"Adding video recordings to slides"},{"location":"components/video/#used-tools","text":"Writing hardware. I.e., a graphics tablet or similar (I used an IPad + Apple Pen with Notability ) A screen recorder (i.e., QuickTime on macOS, screencast on Ubuntu or Zoom) A way to connect the writer with the recorder (I used UxPlay to duplicate the IPad screen on Ubuntu) (Optional) ffmpeg or other tools to edit the video. (Optional) scripts/vmanip.py (Python) to generate ffmpeg commands for cropping the created video.","title":"Used tools"},{"location":"components/video/#steps","text":"Setup the hardware and recoding software and record the video :)","title":"Steps"},{"location":"components/video/#cropping-the-video","text":"The script scripts/vmanip.py allows generating ffmpeg commands which can be used to optimize and crop the video. To continue, you first need to install ffmpeg and pip install -r requirements.txt . The command streamlit run scripts/vmanip.py will create a GUI which can be accessed in your browser (see the printed URL). When following the URL, you can specify a movie, drag-and-drop the boundaries to crop the file, specify a factor to speed up the recording, and eventually copy the corresponding ffmpeg command. For example, for a crop to the box width:height:x:y and a speedup of x1.05 ffmpeg -i media/videos/lecture-10/su2-example.mp4 -filter_complex \"[0]setpts=0.95*PTS[b];[b] crop = 3385:2109:37:28[c];[0]atempo=1.05[a]\" -map \"[c]\" -map \"[a]\" media/videos/lecture-10/su2-example.cropped.mp4 This command needs to be run outside the GUI to create the modified file.","title":"Cropping the video"}]}